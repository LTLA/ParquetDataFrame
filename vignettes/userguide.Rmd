---
title: Using Parquet-backed DataFrames
author:
- name: Aaron Lun
  email: infinite.monkeys.with.keyboards@gmail.com
package: ParquetDataFrame
date: "Revised: October 9, 2023"
output:
  BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{User guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, echo=FALSE}
library(BiocStyle)
self <- Biocpkg("ParquetDataFrame");
knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE)
```

# Overview

The `ParquetDataFrame`, as its name suggests, is a `DataFrame` where the columns are derived from Parquet data.
This is fully file-backed so no data is actually loaded into memory until requested, allowing users to represent large datasets in limited memory.
As the `ParquetDataFrame` inherits from `r Biocpkg("S4Vectors")`' `DataFrame`, it can be used anywhere in Bioconductor's ecosystem that accepts a `DataFrame`, 
e.g., as the column data of a `SummarizedExperiment`, or inside a `BumpyDataFrameMatrix` from the `r Biocpkg("BumpyMatrix")` package.

# Basic usage

Given a path to a Parquet file, we can just supply the path to the `ParquetDataFrame` constructor:

```{r}
# Mocking up a file.
tf <- tempfile()
arrow::write_parquet(cbind(model = rownames(mtcars), mtcars), tf)

# Creating the ParquetDataFrame.
library(ParquetDataFrame)
df <- ParquetDataFrame(tf, key = "model")
df
```

These support all the usual methods for a `DataFrame`, except that the data is kept on disk and referenced as needed:

```{r}
nrow(df)
colnames(df)
class(as.data.frame(df))
```

We extract individual columns as `ParquetColumn` objects.
These are 1-dimensional file-backed `DelayedArray`s that pull a single column's data from the Parquet file on demand.

```{r}
df$mpg

# These can participate in usual vector operations:
df$mpg * 10
log1p(df$mpg)

# Realize this into an ordinary vector.
as.vector(df$mpg)
```

These transformed columns can be assigned back to `ParquetDataFrame` objects.

```{r}
copy <- df
copy$wt <- copy$wt * 1000
top.hits <- head(copy)
top.hits
```

# Retrieving the Arrow Dataset

At any point, users can retrieve the query to the underlying Parquet data via the `arrow_query` method.
This can be used with methods in the `r CRANpkg("arrow")` package to push more complex operations to the C++ libraries for greater efficiency.

```{r}
arrow_query(df)
```

Note that the acquired query will not capture any delayed subsetting/mutation operations that have been applied in the R session.
In theory, it is possible to convert a subset of `r Biocpkg("DelayedArray")` operations into their `r CRANpkg("arrow")` equivalents,
which would improve performance by avoiding the R interpreter when executing a query on the Parquet data.
In practice, any performance boost tends to be rather fragile as only a subset of operations are supported,
meaning that it is easy to silently fall back to R-based evaluation when an unsupported operation is executed.
Users wanting to optimize performance should just operate on the query directly.

# Session information {-}

```{r}
sessionInfo()
```
